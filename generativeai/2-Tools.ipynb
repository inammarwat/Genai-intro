{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfa52177",
   "metadata": {},
   "source": [
    "### Tools\n",
    "Models can request to call tools that perform tasks such as fetching data from a database, searching the web, or running code. Tools are pairings of:\n",
    "1. A schema, including the name of the tool, a description, and/or argument definitions (often a JSON schema)\n",
    "2. A function or coroutine to execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9fd8a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Machine learning models can be categorized in several ways, primarily by **how they learn (learning paradigm)**, **what task they perform (function)**, and their **underlying structure or algorithm family**.\\n\\nHere\\'s a breakdown of the main types:\\n\\n---\\n\\n### 1. By Learning Paradigm (How They Learn)\\n\\nThis is the most fundamental classification.\\n\\n*   **Supervised Learning Models:**\\n    *   **Concept:** Learn from labeled data, meaning the input data is paired with the correct output. The model learns a mapping from input to output.\\n    *   **Goal:** Predict future outcomes for unseen data based on the learned patterns.\\n    *   **Sub-types/Tasks:**\\n        *   **Classification:** Predicts a categorical label (e.g., \"spam\" or \"not spam\", \"cat\" or \"dog\", \"disease A\", \"disease B\", \"healthy\").\\n            *   *Examples:* Logistic Regression, Support Vector Machines (SVM), Decision Trees, Random Forests, K-Nearest Neighbors (k-NN), Neural Networks.\\n        *   **Regression:** Predicts a continuous numerical value (e.g., house prices, stock prices, temperature).\\n            *   *Examples:* Linear Regression, Polynomial Regression, Support Vector Regression (SVR), Decision Trees, Random Forests, Neural Networks.\\n    *   **Use Cases:** Spam detection, medical diagnosis, image recognition, stock price prediction.\\n\\n*   **Unsupervised Learning Models:**\\n    *   **Concept:** Learn from unlabeled data, finding hidden patterns, structures, or relationships within the data without any explicit guidance.\\n    *   **Goal:** Explore data, discover intrinsic structures, or compress data.\\n    *   **Sub-types/Tasks:**\\n        *   **Clustering:** Groups similar data points together (e.g., customer segmentation, document categorization).\\n            *   *Examples:* K-Means, DBSCAN, Hierarchical Clustering.\\n        *   **Dimensionality Reduction:** Reduces the number of features/variables while retaining most of the important information (e.g., for visualization or to speed up other algorithms).\\n            *   *Examples:* Principal Component Analysis (PCA), t-Distributed Stochastic Neighbor Embedding (t-SNE), Autoencoders.\\n        *   **Association Rule Mining:** Discovers relationships between variables in large datasets (e.g., \"people who buy X also tend to buy Y\").\\n            *   *Examples:* Apriori algorithm.\\n    *   **Use Cases:** Customer segmentation, anomaly detection, data compression, recommender systems.\\n\\n*   **Reinforcement Learning Models:**\\n    *   **Concept:** An agent learns by interacting with an environment, performing actions, and receiving rewards or penalties based on those actions. It learns to make a sequence of decisions to maximize cumulative reward.\\n    *   **Goal:** Learn an optimal policy (a strategy) for the agent to behave in an environment.\\n    *   **Components:** Agent, Environment, State, Action, Reward, Policy.\\n    *   *Examples:* Q-learning, SARSA, Deep Q-Networks (DQN), Actor-Critic methods (A2C, PPO).\\n    *   **Use Cases:** Game AI (AlphaGo), robotics, autonomous driving, resource management.\\n\\n*   **Semi-Supervised Learning Models:**\\n    *   **Concept:** Combines a small amount of labeled data with a large amount of unlabeled data during training. It\\'s useful when labeling data is expensive or time-consuming.\\n    *   **Goal:** Leverage the abundant unlabeled data to improve the learning process, often by inferring labels or structures from it.\\n    *   *Examples:* Self-training, Label Propagation, Generative Models (like GANs or VAEs used in semi-supervised settings).\\n    *   **Use Cases:** Web content classification, speech recognition, medical image analysis.\\n\\n*   **Self-Supervised Learning Models (A specialized form of unsupervised learning):**\\n    *   **Concept:** A modern approach where models learn representations from unlabeled data by creating a \"pretext task\" where the data itself provides the supervision signal. The model predicts a missing or corrupted part of its input.\\n    *   **Goal:** Learn powerful feature representations that can then be fine-tuned for downstream supervised tasks with minimal labeled data.\\n    *   *Examples:* Predicting missing words in a sentence (Masked Language Modeling in BERT), predicting the next frame in a video, predicting rotated image angles.\\n    *   **Use Cases:** Pre-training large language models (LLMs), computer vision foundation models.\\n\\n---\\n\\n### 2. By Algorithm Family / Structure\\n\\nThis categorizes models based on their internal mechanics and mathematical foundation.\\n\\n*   **Linear Models:**\\n    *   *Concept:* Models that assume a linear relationship between input features and the output.\\n    *   *Examples:* Linear Regression, Logistic Regression, Ridge Regression, Lasso Regression.\\n\\n*   **Tree-based Models:**\\n    *   *Concept:* Models that partition the data into subsets based on feature values, forming a tree-like structure of decisions.\\n    *   *Examples:* Decision Trees, Random Forests (ensemble of decision trees), Gradient Boosting Machines (XGBoost, LightGBM, CatBoost).\\n\\n*   **Support Vector Machines (SVMs):**\\n    *   *Concept:* Find an optimal hyperplane that best separates different classes in the feature space. Can use kernels to handle non-linear relationships.\\n    *   *Examples:* Support Vector Classifier (SVC), Support Vector Regressor (SVR).\\n\\n*   **K-Nearest Neighbors (k-NN):**\\n    *   *Concept:* An instance-based (non-parametric) model that classifies or predicts based on the majority class or average value of its \\'k\\' nearest neighbors in the feature space.\\n\\n*   **Bayesian Models:**\\n    *   *Concept:* Models based on Bayes\\' Theorem, which uses probabilities to make predictions.\\n    *   *Examples:* Naive Bayes (Gaussian, Multinomial, Bernoulli).\\n\\n*   **Neural Networks (Deep Learning Models):**\\n    *   *Concept:* Inspired by the human brain, these models consist of interconnected \"neurons\" organized in layers. They can learn complex non-linear relationships. \"Deep\" implies many layers.\\n    *   *Examples:*\\n        *   **Feedforward Neural Networks (FNNs) / Multi-Layer Perceptrons (MLPs):** Basic, fully connected networks.\\n        *   **Convolutional Neural Networks (CNNs):** Excellent for image and video processing due to their ability to detect spatial hierarchies of features.\\n        *   **Recurrent Neural Networks (RNNs):** Designed for sequential data (e.g., text, time series) due to their internal memory (LSTMs, GRUs are advanced variants).\\n        *   **Transformers:** Revolutionized NLP and increasingly used in computer vision, relying on \"attention mechanisms\" to weigh the importance of different parts of the input. (e.g., BERT, GPT).\\n        *   **Generative Adversarial Networks (GANs):** Two neural networks (generator and discriminator) compete to generate realistic new data (e.g., images, audio).\\n        *   **Variational Autoencoders (VAEs):** Generative models that learn a compressed representation (latent space) of the input data.\\n        *   **Diffusion Models:** State-of-the-art generative models that iteratively denoise data to create new samples.\\n\\n*   **Ensemble Models:**\\n    *   *Concept:* Combine multiple individual models (often called \"weak learners\") to produce a more robust and accurate prediction than any single model could achieve.\\n    *   *Examples:*\\n        *   **Bagging:** (e.g., Random Forests) Trains multiple models independently on different subsets of the data and averages their predictions.\\n        *   **Boosting:** (e.g., AdaBoost, Gradient Boosting, XGBoost) Trains models sequentially, with each new model focusing on correcting the errors of the previous ones.\\n        *   **Stacking:** Combines predictions from multiple models using another model (a meta-learner) to make the final prediction.\\n\\n---\\n\\nThe choice of which model type to use depends heavily on the specific problem, the nature of the data, the desired interpretability, and available computational resources.', additional_kwargs={}, response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'model_provider': 'google_genai'}, id='lc_run--019be445-7304-7881-a154-c2719fa379bf-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 7, 'output_tokens': 3492, 'total_tokens': 3499, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 1731}})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the os module to work with environment variables\n",
    "import os\n",
    "\n",
    "# Import the function to initialize a chat-based LLM from LangChain\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "# Load the Google API key from system environment variables\n",
    "# Make sure GOOGLE_API_KEY is already set in your system\n",
    "os.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "# Initialize the Gemini 2.5 Flash model using Google's Generative AI provider\n",
    "# This model will be used for chat-based question answering\n",
    "model = init_chat_model(\n",
    "    \"gemini-2.5-flash\",\n",
    "    model_provider=\"google_genai\"\n",
    ")\n",
    "\n",
    "# Send a prompt (question) to the model\n",
    "# The invoke() method sends the input and receives the AI-generated response\n",
    "response = model.invoke(\"Types of Machine Learning Model?\")\n",
    "\n",
    "# Print or return the model's response\n",
    "response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b1b13c",
   "metadata": {},
   "source": [
    "### Enabling External Tools in LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f56f805",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "# 1. Define the Tool using a Decorator\n",
    "# The @tool decorator converts a standard Python function into a LangChain Tool.\n",
    "# The AI uses the function name, arguments, and the docstring (\"Get the weather...\") \n",
    "# to understand when and how to use this tool.\n",
    "@tool\n",
    "def get_weather(location: str) -> str:\n",
    "    \"\"\"Get the weather at a location\"\"\"\n",
    "    return f\"it's sunny in {location}\"\n",
    "\n",
    "# 2. Bind the Tool to the Model\n",
    "# model.bind_tools() takes your list of functions and tells the Gemini model:\n",
    "# \"You now have permission to ask to run these specific functions.\"\n",
    "model_with_tools = model.bind_tools([get_weather])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5161b420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='' additional_kwargs={'function_call': {'name': 'get_weather', 'arguments': '{\"location\": \"Boston\"}'}, '__gemini_function_call_thought_signatures__': {'34e94a70-b7d3-44c6-b5e0-9f9f315d3c29': 'CukBAXLI2nx94XIs3sxi3bbsXlQuCZsXMXf+YEjzwI2nVBiB4LH0SMl/mXnaA0nzNkRCIREg+ouzx0JaeisrN5ETEmJxdua40aIq+kLQEsEwr4qwOO0LlBQWuCprxq3wWFI9PYKKqe17ZXlQaV4Tmk5pmUuHUThi9y2WUWJGNYKucrcZq/X1SQMY8uM0myHDGzSLKCNWymf+Paew6OO16dAym5mh8k+nBabHclyqG1hk8Me3EQYAGMxG/eUN32pNOLTLk+uxMB1yToqLohrQDBhurp7IX9t4b2bjuH2dtxE4DsPxMzdvD9qL7Fc='}} response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'model_provider': 'google_genai'} id='lc_run--019be4d7-aaed-77b0-aac3-3937751fedc0-0' tool_calls=[{'name': 'get_weather', 'args': {'location': 'Boston'}, 'id': '34e94a70-b7d3-44c6-b5e0-9f9f315d3c29', 'type': 'tool_call'}] invalid_tool_calls=[] usage_metadata={'input_tokens': 48, 'output_tokens': 61, 'total_tokens': 109, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 46}}\n",
      "Tool: get_weather\n",
      "Args: {'location': 'Boston'}\n"
     ]
    }
   ],
   "source": [
    "response = model_with_tools.invoke(\"What's the weather like in Boston?\")\n",
    "print(response)\n",
    "for tool_call in response.tool_calls:\n",
    "    # View tool calls made by the model\n",
    "    print(f\"Tool: {tool_call['name']}\")\n",
    "    print(f\"Args: {tool_call['args']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd38908e",
   "metadata": {},
   "source": [
    "### Tool Execution Loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08528199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's sunny in Boston.\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Model generates tool calls\n",
    "messages = [{\"role\": \"user\", \"content\": \"What's the weather in Boston?\"}]\n",
    "ai_msg = model_with_tools.invoke(messages)\n",
    "messages.append(ai_msg)\n",
    "\n",
    "# Step 2: Execute tools and collect results\n",
    "for tool_call in ai_msg.tool_calls:\n",
    "    # Execute the tool with the generated arguments\n",
    "    tool_result = get_weather.invoke(tool_call)\n",
    "    messages.append(tool_result)\n",
    "\n",
    "# Step 3: Pass results back to model for final response\n",
    "final_response = model_with_tools.invoke(messages)\n",
    "print(final_response.text)\n",
    "# \"The current weather in Boston is 72Â°F and sunny.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33e0f2b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': \"What's the weather in Boston?\"},\n",
       " AIMessage(content='', additional_kwargs={'function_call': {'name': 'get_weather', 'arguments': '{\"location\": \"Boston\"}'}, '__gemini_function_call_thought_signatures__': {'b32b6cf0-5507-4615-8e24-c9da2e9e7a2c': 'CukBAXLI2nz5pko0VxBP5TQdxPbscPdKI6whjTj5FRcHUmLG6nb0VVfywlnZNVQmNhFdbd5JJuRcW7dDhQQ4z2bWNEmExs6M3c2G2ki1A+M9ert5gWfBOkDCT5TSWbhrv5+9jci218JA9iHEVazs2ctLPNSjt1BoEiNfqDQmVqEbVx+6Nl7bISUvT6fwi3b11jAbV6FCFe+sTtuxbCAGnjxLIy4YsD5Y0tdp/3ne3fOB0bYfcj7xwauOm/OW6IXYW6BJlk8cTB3aqQTxa8iUav7s3c91+MuFXtHtf2X7WVOIReHLDM587z6x60Q='}}, response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'model_provider': 'google_genai'}, id='lc_run--019be4d9-104f-77d3-a30e-1f6bc487f3cb-0', tool_calls=[{'name': 'get_weather', 'args': {'location': 'Boston'}, 'id': 'b32b6cf0-5507-4615-8e24-c9da2e9e7a2c', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 47, 'output_tokens': 61, 'total_tokens': 108, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 46}}),\n",
       " ToolMessage(content=\"it's sunny in Boston\", name='get_weather', tool_call_id='b32b6cf0-5507-4615-8e24-c9da2e9e7a2c')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
